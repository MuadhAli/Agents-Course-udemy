{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key exists and begins AI\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefix to help with any debugging\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "    genai.configure(api_key=google_api_key)\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is required)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.\n"
     ]
    }
   ],
   "source": [
    "print(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyze the ethical implications of using advanced AI to predict and potentially prevent individual acts of violence, considering both the potential for reducing harm and the inherent risks to individual liberties and the potential for biased outcomes.  Discuss the trade-offs and propose a framework for responsible implementation, acknowledging the complexities and lack of easy answers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use Gemini to generate the question\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(request)\n",
    "question = response.text\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## The Ethical Tightrope: AI Prediction and Prevention of Violence\n",
       "\n",
       "The use of advanced AI to predict and prevent individual acts of violence presents a complex ethical dilemma, fraught with potential benefits and significant risks.  On one hand, it offers the tantalizing prospect of reducing harm and saving lives. On the other, it raises profound concerns about individual liberties, fairness, and the potential for misuse.  \n",
       "\n",
       "**Potential Benefits (Reducing Harm):**\n",
       "\n",
       "* **Proactive Intervention:** AI could analyze vast datasets (social media activity, past behavior, environmental factors) to identify individuals at high risk of committing violence, enabling timely intervention through mental health services, support networks, or law enforcement.\n",
       "* **Resource Allocation:**  By prioritizing individuals identified as high-risk, limited resources (police, social workers) could be allocated more effectively, potentially preventing more acts of violence than reactive approaches.\n",
       "* **Early Warning System:**  AI could provide early warnings of potential threats, allowing authorities to take preventative measures before violence occurs, potentially averting mass casualty events.\n",
       "\n",
       "\n",
       "**Inherent Risks (Threats to Liberties and Biased Outcomes):**\n",
       "\n",
       "* **False Positives:**  AI models are prone to errors.  A high rate of false positives could lead to the stigmatization, harassment, and unnecessary surveillance of innocent individuals, severely impacting their lives and reputations.  This is particularly concerning for marginalized communities.\n",
       "* **Bias and Discrimination:** AI models are trained on data, and if that data reflects existing societal biases (racial, socioeconomic, etc.), the resulting predictions will likely perpetuate and amplify these biases, leading to disproportionate targeting of certain groups.\n",
       "* **Erosion of Privacy:**  The collection and analysis of massive datasets required for accurate prediction inevitably raises serious privacy concerns.  The potential for abuse of this sensitive information is substantial.\n",
       "* **Preemptive Punishment:**  Predictive policing, based on AI, raises the specter of preemptive punishment â€“ punishing individuals for crimes they might commit, rather than crimes they have actually committed.  This is fundamentally at odds with core principles of justice.\n",
       "* **Lack of Transparency and Accountability:**  The \"black box\" nature of many AI algorithms makes it difficult to understand how predictions are made, hindering accountability and making it hard to identify and correct biases.\n",
       "\n",
       "\n",
       "**Trade-offs and a Framework for Responsible Implementation:**\n",
       "\n",
       "The trade-off lies between maximizing public safety and protecting individual rights and freedoms.  There is no easy answer, but a responsible approach necessitates a multi-faceted framework:\n",
       "\n",
       "1. **Data Transparency and Bias Mitigation:**  Rigorous auditing of datasets used to train AI models is crucial.  Techniques to identify and mitigate biases should be employed, and the methodology for model development and validation needs to be transparent and auditable.\n",
       "2. **Human Oversight and Due Process:**  AI predictions should not be the sole basis for intervention.  Human oversight is essential, with clear guidelines and procedures for reviewing AI-generated risk assessments and ensuring due process rights are respected.\n",
       "3. **Focus on Risk Reduction, not Prediction Alone:**  The goal should be to reduce the risk of violence, not simply to predict it.  Interventions should focus on providing support and resources to individuals identified as at-risk, rather than simply labeling and surveilling them.\n",
       "4. **Ethical Review Boards and Public Engagement:**  Independent ethical review boards should oversee the development and deployment of AI systems for violence prediction, ensuring that they align with ethical principles and societal values.  Public engagement and debate are essential for establishing societal consensus on acceptable levels of risk and intervention.\n",
       "5. **Continuous Monitoring and Evaluation:**  The performance of AI systems should be continuously monitored and evaluated to identify and address biases, inaccuracies, and unintended consequences.  Mechanisms for redress and accountability must be in place.\n",
       "\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "The use of AI to predict and prevent violence offers significant potential benefits, but it presents substantial ethical challenges.  A responsible approach demands a cautious and deliberative process that prioritizes both public safety and fundamental human rights.  This requires transparency, accountability, robust ethical oversight, and a commitment to ongoing monitoring and refinement of these powerful technologies.  The path forward requires careful consideration of these complex issues and a willingness to engage in ongoing dialogue among policymakers, ethicists, technologists, and the public.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call Gemini with different models (flash and pro)\n",
    "for model_name in [\"gemini-1.5-flash\"]:\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    answer_response = model.generate_content(question)\n",
    "    answer = answer_response.text\n",
    "    display(Markdown(answer))\n",
    "    competitors.append(model_name)\n",
    "    answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All responses above are from Gemini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gemini-1.5-flash', 'gemini-1.5-flash']\n",
      "['## The Ethical Tightrope: AI Prediction and Prevention of Violence\\n\\nThe use of advanced AI to predict and prevent individual acts of violence presents a profound ethical dilemma. While the potential for reducing harm is undeniable, the inherent risks to individual liberties and the potential for biased outcomes create a complex ethical landscape demanding careful consideration.\\n\\n**Potential Benefits (Reducing Harm):**\\n\\n* **Early Intervention:** AI could analyze vast datasets (social media activity, criminal records, mental health indicators) to identify individuals at high risk of committing violence, allowing for proactive interventions like therapy, counseling, or increased monitoring.\\n* **Resource Allocation:**  Predictive models could help law enforcement and social services allocate resources more effectively, focusing on individuals who pose the greatest risk.\\n* **Improved Public Safety:** By preventing acts of violence, AI could contribute to a safer society for everyone.\\n\\n**Ethical Risks & Trade-offs:**\\n\\n* **Violation of Privacy:** The collection and analysis of personal data required for predictive models raise serious privacy concerns. Individuals may be subjected to surveillance without their knowledge or consent, leading to a chilling effect on freedom of expression and association.\\n* **False Positives & Stigmatization:**  Inaccurate predictions could lead to the wrongful labeling and stigmatization of individuals, causing significant emotional distress and social harm.  This is particularly problematic for marginalized groups who are disproportionately targeted by biased algorithms.\\n* **Bias and Discrimination:** AI models are trained on data reflecting existing societal biases. This can lead to discriminatory outcomes, where certain groups are unfairly flagged as high-risk, perpetuating existing inequalities.\\n* **Preemptive Punishment:**  Predictive policing, even with accurate predictions, raises concerns about preemptive punishment. Individuals are potentially penalized for actions they may never commit, violating fundamental principles of justice.\\n* **Erosion of Due Process:**  Relying on AI predictions to inform decisions related to arrest, detention, or other legal actions could undermine due process and fair trial rights.\\n* **Lack of Transparency and Accountability:**  The complexity of AI algorithms often makes it difficult to understand how predictions are generated, making it challenging to identify and correct biases or hold systems accountable for errors.\\n\\n\\n**Framework for Responsible Implementation:**\\n\\nA responsible approach to implementing AI for violence prediction requires a multi-faceted strategy:\\n\\n1. **Data Governance and Privacy Protection:**  Establish strict data governance protocols ensuring data anonymization, minimization, and secure storage.  Implement robust mechanisms for consent and oversight, including independent audits of data collection and use.\\n2. **Algorithmic Transparency and Explainability:**  Develop AI models that are transparent and explainable, enabling scrutiny of their decision-making processes and identification of biases.  This involves prioritizing interpretable models over highly accurate but opaque \"black box\" systems.\\n3. **Bias Mitigation and Fairness:**  Actively address bias in data and algorithms through rigorous testing and validation procedures.  Employ techniques like fairness-aware machine learning and incorporate diverse perspectives in model development and evaluation.\\n4. **Human Oversight and Accountability:**  Ensure that AI predictions are not used as the sole basis for decision-making.  Establish clear guidelines for human oversight and review of AI-generated predictions, emphasizing the importance of individual rights and due process.\\n5. **Public Engagement and Education:**  Foster open dialogue and public engagement to build trust and address concerns about the use of AI in this sensitive area.  Educate the public about the capabilities and limitations of AI predictive tools.\\n6. **Continuous Monitoring and Evaluation:**  Regularly monitor the performance of AI systems and assess their impact on individuals and communities.  Implement mechanisms for feedback and iterative improvement based on real-world experience.\\n\\n\\n**Conclusion:**\\n\\nThe ethical implications of using AI to predict and prevent violence are complex and far-reaching.  There are no easy answers, and the potential benefits must be carefully weighed against the inherent risks.  A responsible implementation requires a commitment to transparency, accountability, fairness, and respect for individual liberties.  This necessitates a collaborative effort involving policymakers, technologists, ethicists, and community stakeholders to develop and implement a framework that prioritizes both public safety and individual rights.  The goal should be to leverage the potential of AI while mitigating its risks, ensuring that technology serves humanity rather than exacerbating existing inequalities and undermining fundamental freedoms.\\n', '## The Ethical Tightrope: AI Prediction and Prevention of Violence\\n\\nThe use of advanced AI to predict and prevent individual acts of violence presents a complex ethical dilemma, fraught with potential benefits and significant risks.  On one hand, it offers the tantalizing prospect of reducing harm and saving lives. On the other, it raises profound concerns about individual liberties, fairness, and the potential for misuse.  \\n\\n**Potential Benefits (Reducing Harm):**\\n\\n* **Proactive Intervention:** AI could analyze vast datasets (social media activity, past behavior, environmental factors) to identify individuals at high risk of committing violence, enabling timely intervention through mental health services, support networks, or law enforcement.\\n* **Resource Allocation:**  By prioritizing individuals identified as high-risk, limited resources (police, social workers) could be allocated more effectively, potentially preventing more acts of violence than reactive approaches.\\n* **Early Warning System:**  AI could provide early warnings of potential threats, allowing authorities to take preventative measures before violence occurs, potentially averting mass casualty events.\\n\\n\\n**Inherent Risks (Threats to Liberties and Biased Outcomes):**\\n\\n* **False Positives:**  AI models are prone to errors.  A high rate of false positives could lead to the stigmatization, harassment, and unnecessary surveillance of innocent individuals, severely impacting their lives and reputations.  This is particularly concerning for marginalized communities.\\n* **Bias and Discrimination:** AI models are trained on data, and if that data reflects existing societal biases (racial, socioeconomic, etc.), the resulting predictions will likely perpetuate and amplify these biases, leading to disproportionate targeting of certain groups.\\n* **Erosion of Privacy:**  The collection and analysis of massive datasets required for accurate prediction inevitably raises serious privacy concerns.  The potential for abuse of this sensitive information is substantial.\\n* **Preemptive Punishment:**  Predictive policing, based on AI, raises the specter of preemptive punishment â€“ punishing individuals for crimes they might commit, rather than crimes they have actually committed.  This is fundamentally at odds with core principles of justice.\\n* **Lack of Transparency and Accountability:**  The \"black box\" nature of many AI algorithms makes it difficult to understand how predictions are made, hindering accountability and making it hard to identify and correct biases.\\n\\n\\n**Trade-offs and a Framework for Responsible Implementation:**\\n\\nThe trade-off lies between maximizing public safety and protecting individual rights and freedoms.  There is no easy answer, but a responsible approach necessitates a multi-faceted framework:\\n\\n1. **Data Transparency and Bias Mitigation:**  Rigorous auditing of datasets used to train AI models is crucial.  Techniques to identify and mitigate biases should be employed, and the methodology for model development and validation needs to be transparent and auditable.\\n2. **Human Oversight and Due Process:**  AI predictions should not be the sole basis for intervention.  Human oversight is essential, with clear guidelines and procedures for reviewing AI-generated risk assessments and ensuring due process rights are respected.\\n3. **Focus on Risk Reduction, not Prediction Alone:**  The goal should be to reduce the risk of violence, not simply to predict it.  Interventions should focus on providing support and resources to individuals identified as at-risk, rather than simply labeling and surveilling them.\\n4. **Ethical Review Boards and Public Engagement:**  Independent ethical review boards should oversee the development and deployment of AI systems for violence prediction, ensuring that they align with ethical principles and societal values.  Public engagement and debate are essential for establishing societal consensus on acceptable levels of risk and intervention.\\n5. **Continuous Monitoring and Evaluation:**  The performance of AI systems should be continuously monitored and evaluated to identify and address biases, inaccuracies, and unintended consequences.  Mechanisms for redress and accountability must be in place.\\n\\n\\n**Conclusion:**\\n\\nThe use of AI to predict and prevent violence offers significant potential benefits, but it presents substantial ethical challenges.  A responsible approach demands a cautious and deliberative process that prioritizes both public safety and fundamental human rights.  This requires transparency, accountability, robust ethical oversight, and a commitment to ongoing monitoring and refinement of these powerful technologies.  The path forward requires careful consideration of these complex issues and a willingness to engage in ongoing dialogue among policymakers, ethicists, technologists, and the public.\\n']\n"
     ]
    }
   ],
   "source": [
    "# So where are we?\n",
    "print(competitors)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competitor: gemini-1.5-flash\n",
      "\n",
      "## The Ethical Tightrope: AI Prediction and Prevention of Violence\n",
      "\n",
      "The use of advanced AI to predict and prevent individual acts of violence presents a profound ethical dilemma. While the potential for reducing harm is undeniable, the inherent risks to individual liberties and the potential for biased outcomes create a complex ethical landscape demanding careful consideration.\n",
      "\n",
      "**Potential Benefits (Reducing Harm):**\n",
      "\n",
      "* **Early Intervention:** AI could analyze vast datasets (social media activity, criminal records, mental health indicators) to identify individuals at high risk of committing violence, allowing for proactive interventions like therapy, counseling, or increased monitoring.\n",
      "* **Resource Allocation:**  Predictive models could help law enforcement and social services allocate resources more effectively, focusing on individuals who pose the greatest risk.\n",
      "* **Improved Public Safety:** By preventing acts of violence, AI could contribute to a safer society for everyone.\n",
      "\n",
      "**Ethical Risks & Trade-offs:**\n",
      "\n",
      "* **Violation of Privacy:** The collection and analysis of personal data required for predictive models raise serious privacy concerns. Individuals may be subjected to surveillance without their knowledge or consent, leading to a chilling effect on freedom of expression and association.\n",
      "* **False Positives & Stigmatization:**  Inaccurate predictions could lead to the wrongful labeling and stigmatization of individuals, causing significant emotional distress and social harm.  This is particularly problematic for marginalized groups who are disproportionately targeted by biased algorithms.\n",
      "* **Bias and Discrimination:** AI models are trained on data reflecting existing societal biases. This can lead to discriminatory outcomes, where certain groups are unfairly flagged as high-risk, perpetuating existing inequalities.\n",
      "* **Preemptive Punishment:**  Predictive policing, even with accurate predictions, raises concerns about preemptive punishment. Individuals are potentially penalized for actions they may never commit, violating fundamental principles of justice.\n",
      "* **Erosion of Due Process:**  Relying on AI predictions to inform decisions related to arrest, detention, or other legal actions could undermine due process and fair trial rights.\n",
      "* **Lack of Transparency and Accountability:**  The complexity of AI algorithms often makes it difficult to understand how predictions are generated, making it challenging to identify and correct biases or hold systems accountable for errors.\n",
      "\n",
      "\n",
      "**Framework for Responsible Implementation:**\n",
      "\n",
      "A responsible approach to implementing AI for violence prediction requires a multi-faceted strategy:\n",
      "\n",
      "1. **Data Governance and Privacy Protection:**  Establish strict data governance protocols ensuring data anonymization, minimization, and secure storage.  Implement robust mechanisms for consent and oversight, including independent audits of data collection and use.\n",
      "2. **Algorithmic Transparency and Explainability:**  Develop AI models that are transparent and explainable, enabling scrutiny of their decision-making processes and identification of biases.  This involves prioritizing interpretable models over highly accurate but opaque \"black box\" systems.\n",
      "3. **Bias Mitigation and Fairness:**  Actively address bias in data and algorithms through rigorous testing and validation procedures.  Employ techniques like fairness-aware machine learning and incorporate diverse perspectives in model development and evaluation.\n",
      "4. **Human Oversight and Accountability:**  Ensure that AI predictions are not used as the sole basis for decision-making.  Establish clear guidelines for human oversight and review of AI-generated predictions, emphasizing the importance of individual rights and due process.\n",
      "5. **Public Engagement and Education:**  Foster open dialogue and public engagement to build trust and address concerns about the use of AI in this sensitive area.  Educate the public about the capabilities and limitations of AI predictive tools.\n",
      "6. **Continuous Monitoring and Evaluation:**  Regularly monitor the performance of AI systems and assess their impact on individuals and communities.  Implement mechanisms for feedback and iterative improvement based on real-world experience.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The ethical implications of using AI to predict and prevent violence are complex and far-reaching.  There are no easy answers, and the potential benefits must be carefully weighed against the inherent risks.  A responsible implementation requires a commitment to transparency, accountability, fairness, and respect for individual liberties.  This necessitates a collaborative effort involving policymakers, technologists, ethicists, and community stakeholders to develop and implement a framework that prioritizes both public safety and individual rights.  The goal should be to leverage the potential of AI while mitigating its risks, ensuring that technology serves humanity rather than exacerbating existing inequalities and undermining fundamental freedoms.\n",
      "\n",
      "Competitor: gemini-1.5-flash\n",
      "\n",
      "## The Ethical Tightrope: AI Prediction and Prevention of Violence\n",
      "\n",
      "The use of advanced AI to predict and prevent individual acts of violence presents a complex ethical dilemma, fraught with potential benefits and significant risks.  On one hand, it offers the tantalizing prospect of reducing harm and saving lives. On the other, it raises profound concerns about individual liberties, fairness, and the potential for misuse.  \n",
      "\n",
      "**Potential Benefits (Reducing Harm):**\n",
      "\n",
      "* **Proactive Intervention:** AI could analyze vast datasets (social media activity, past behavior, environmental factors) to identify individuals at high risk of committing violence, enabling timely intervention through mental health services, support networks, or law enforcement.\n",
      "* **Resource Allocation:**  By prioritizing individuals identified as high-risk, limited resources (police, social workers) could be allocated more effectively, potentially preventing more acts of violence than reactive approaches.\n",
      "* **Early Warning System:**  AI could provide early warnings of potential threats, allowing authorities to take preventative measures before violence occurs, potentially averting mass casualty events.\n",
      "\n",
      "\n",
      "**Inherent Risks (Threats to Liberties and Biased Outcomes):**\n",
      "\n",
      "* **False Positives:**  AI models are prone to errors.  A high rate of false positives could lead to the stigmatization, harassment, and unnecessary surveillance of innocent individuals, severely impacting their lives and reputations.  This is particularly concerning for marginalized communities.\n",
      "* **Bias and Discrimination:** AI models are trained on data, and if that data reflects existing societal biases (racial, socioeconomic, etc.), the resulting predictions will likely perpetuate and amplify these biases, leading to disproportionate targeting of certain groups.\n",
      "* **Erosion of Privacy:**  The collection and analysis of massive datasets required for accurate prediction inevitably raises serious privacy concerns.  The potential for abuse of this sensitive information is substantial.\n",
      "* **Preemptive Punishment:**  Predictive policing, based on AI, raises the specter of preemptive punishment â€“ punishing individuals for crimes they might commit, rather than crimes they have actually committed.  This is fundamentally at odds with core principles of justice.\n",
      "* **Lack of Transparency and Accountability:**  The \"black box\" nature of many AI algorithms makes it difficult to understand how predictions are made, hindering accountability and making it hard to identify and correct biases.\n",
      "\n",
      "\n",
      "**Trade-offs and a Framework for Responsible Implementation:**\n",
      "\n",
      "The trade-off lies between maximizing public safety and protecting individual rights and freedoms.  There is no easy answer, but a responsible approach necessitates a multi-faceted framework:\n",
      "\n",
      "1. **Data Transparency and Bias Mitigation:**  Rigorous auditing of datasets used to train AI models is crucial.  Techniques to identify and mitigate biases should be employed, and the methodology for model development and validation needs to be transparent and auditable.\n",
      "2. **Human Oversight and Due Process:**  AI predictions should not be the sole basis for intervention.  Human oversight is essential, with clear guidelines and procedures for reviewing AI-generated risk assessments and ensuring due process rights are respected.\n",
      "3. **Focus on Risk Reduction, not Prediction Alone:**  The goal should be to reduce the risk of violence, not simply to predict it.  Interventions should focus on providing support and resources to individuals identified as at-risk, rather than simply labeling and surveilling them.\n",
      "4. **Ethical Review Boards and Public Engagement:**  Independent ethical review boards should oversee the development and deployment of AI systems for violence prediction, ensuring that they align with ethical principles and societal values.  Public engagement and debate are essential for establishing societal consensus on acceptable levels of risk and intervention.\n",
      "5. **Continuous Monitoring and Evaluation:**  The performance of AI systems should be continuously monitored and evaluated to identify and address biases, inaccuracies, and unintended consequences.  Mechanisms for redress and accountability must be in place.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The use of AI to predict and prevent violence offers significant potential benefits, but it presents substantial ethical challenges.  A responsible approach demands a cautious and deliberative process that prioritizes both public safety and fundamental human rights.  This requires transparency, accountability, robust ethical oversight, and a commitment to ongoing monitoring and refinement of these powerful technologies.  The path forward requires careful consideration of these complex issues and a willingness to engage in ongoing dialogue among policymakers, ethicists, technologists, and the public.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Response from competitor 1\n",
      "\n",
      "## The Ethical Tightrope: AI Prediction and Prevention of Violence\n",
      "\n",
      "The use of advanced AI to predict and prevent individual acts of violence presents a profound ethical dilemma. While the potential for reducing harm is undeniable, the inherent risks to individual liberties and the potential for biased outcomes create a complex ethical landscape demanding careful consideration.\n",
      "\n",
      "**Potential Benefits (Reducing Harm):**\n",
      "\n",
      "* **Early Intervention:** AI could analyze vast datasets (social media activity, criminal records, mental health indicators) to identify individuals at high risk of committing violence, allowing for proactive interventions like therapy, counseling, or increased monitoring.\n",
      "* **Resource Allocation:**  Predictive models could help law enforcement and social services allocate resources more effectively, focusing on individuals who pose the greatest risk.\n",
      "* **Improved Public Safety:** By preventing acts of violence, AI could contribute to a safer society for everyone.\n",
      "\n",
      "**Ethical Risks & Trade-offs:**\n",
      "\n",
      "* **Violation of Privacy:** The collection and analysis of personal data required for predictive models raise serious privacy concerns. Individuals may be subjected to surveillance without their knowledge or consent, leading to a chilling effect on freedom of expression and association.\n",
      "* **False Positives & Stigmatization:**  Inaccurate predictions could lead to the wrongful labeling and stigmatization of individuals, causing significant emotional distress and social harm.  This is particularly problematic for marginalized groups who are disproportionately targeted by biased algorithms.\n",
      "* **Bias and Discrimination:** AI models are trained on data reflecting existing societal biases. This can lead to discriminatory outcomes, where certain groups are unfairly flagged as high-risk, perpetuating existing inequalities.\n",
      "* **Preemptive Punishment:**  Predictive policing, even with accurate predictions, raises concerns about preemptive punishment. Individuals are potentially penalized for actions they may never commit, violating fundamental principles of justice.\n",
      "* **Erosion of Due Process:**  Relying on AI predictions to inform decisions related to arrest, detention, or other legal actions could undermine due process and fair trial rights.\n",
      "* **Lack of Transparency and Accountability:**  The complexity of AI algorithms often makes it difficult to understand how predictions are generated, making it challenging to identify and correct biases or hold systems accountable for errors.\n",
      "\n",
      "\n",
      "**Framework for Responsible Implementation:**\n",
      "\n",
      "A responsible approach to implementing AI for violence prediction requires a multi-faceted strategy:\n",
      "\n",
      "1. **Data Governance and Privacy Protection:**  Establish strict data governance protocols ensuring data anonymization, minimization, and secure storage.  Implement robust mechanisms for consent and oversight, including independent audits of data collection and use.\n",
      "2. **Algorithmic Transparency and Explainability:**  Develop AI models that are transparent and explainable, enabling scrutiny of their decision-making processes and identification of biases.  This involves prioritizing interpretable models over highly accurate but opaque \"black box\" systems.\n",
      "3. **Bias Mitigation and Fairness:**  Actively address bias in data and algorithms through rigorous testing and validation procedures.  Employ techniques like fairness-aware machine learning and incorporate diverse perspectives in model development and evaluation.\n",
      "4. **Human Oversight and Accountability:**  Ensure that AI predictions are not used as the sole basis for decision-making.  Establish clear guidelines for human oversight and review of AI-generated predictions, emphasizing the importance of individual rights and due process.\n",
      "5. **Public Engagement and Education:**  Foster open dialogue and public engagement to build trust and address concerns about the use of AI in this sensitive area.  Educate the public about the capabilities and limitations of AI predictive tools.\n",
      "6. **Continuous Monitoring and Evaluation:**  Regularly monitor the performance of AI systems and assess their impact on individuals and communities.  Implement mechanisms for feedback and iterative improvement based on real-world experience.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The ethical implications of using AI to predict and prevent violence are complex and far-reaching.  There are no easy answers, and the potential benefits must be carefully weighed against the inherent risks.  A responsible implementation requires a commitment to transparency, accountability, fairness, and respect for individual liberties.  This necessitates a collaborative effort involving policymakers, technologists, ethicists, and community stakeholders to develop and implement a framework that prioritizes both public safety and individual rights.  The goal should be to leverage the potential of AI while mitigating its risks, ensuring that technology serves humanity rather than exacerbating existing inequalities and undermining fundamental freedoms.\n",
      "\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "## The Ethical Tightrope: AI Prediction and Prevention of Violence\n",
      "\n",
      "The use of advanced AI to predict and prevent individual acts of violence presents a complex ethical dilemma, fraught with potential benefits and significant risks.  On one hand, it offers the tantalizing prospect of reducing harm and saving lives. On the other, it raises profound concerns about individual liberties, fairness, and the potential for misuse.  \n",
      "\n",
      "**Potential Benefits (Reducing Harm):**\n",
      "\n",
      "* **Proactive Intervention:** AI could analyze vast datasets (social media activity, past behavior, environmental factors) to identify individuals at high risk of committing violence, enabling timely intervention through mental health services, support networks, or law enforcement.\n",
      "* **Resource Allocation:**  By prioritizing individuals identified as high-risk, limited resources (police, social workers) could be allocated more effectively, potentially preventing more acts of violence than reactive approaches.\n",
      "* **Early Warning System:**  AI could provide early warnings of potential threats, allowing authorities to take preventative measures before violence occurs, potentially averting mass casualty events.\n",
      "\n",
      "\n",
      "**Inherent Risks (Threats to Liberties and Biased Outcomes):**\n",
      "\n",
      "* **False Positives:**  AI models are prone to errors.  A high rate of false positives could lead to the stigmatization, harassment, and unnecessary surveillance of innocent individuals, severely impacting their lives and reputations.  This is particularly concerning for marginalized communities.\n",
      "* **Bias and Discrimination:** AI models are trained on data, and if that data reflects existing societal biases (racial, socioeconomic, etc.), the resulting predictions will likely perpetuate and amplify these biases, leading to disproportionate targeting of certain groups.\n",
      "* **Erosion of Privacy:**  The collection and analysis of massive datasets required for accurate prediction inevitably raises serious privacy concerns.  The potential for abuse of this sensitive information is substantial.\n",
      "* **Preemptive Punishment:**  Predictive policing, based on AI, raises the specter of preemptive punishment â€“ punishing individuals for crimes they might commit, rather than crimes they have actually committed.  This is fundamentally at odds with core principles of justice.\n",
      "* **Lack of Transparency and Accountability:**  The \"black box\" nature of many AI algorithms makes it difficult to understand how predictions are made, hindering accountability and making it hard to identify and correct biases.\n",
      "\n",
      "\n",
      "**Trade-offs and a Framework for Responsible Implementation:**\n",
      "\n",
      "The trade-off lies between maximizing public safety and protecting individual rights and freedoms.  There is no easy answer, but a responsible approach necessitates a multi-faceted framework:\n",
      "\n",
      "1. **Data Transparency and Bias Mitigation:**  Rigorous auditing of datasets used to train AI models is crucial.  Techniques to identify and mitigate biases should be employed, and the methodology for model development and validation needs to be transparent and auditable.\n",
      "2. **Human Oversight and Due Process:**  AI predictions should not be the sole basis for intervention.  Human oversight is essential, with clear guidelines and procedures for reviewing AI-generated risk assessments and ensuring due process rights are respected.\n",
      "3. **Focus on Risk Reduction, not Prediction Alone:**  The goal should be to reduce the risk of violence, not simply to predict it.  Interventions should focus on providing support and resources to individuals identified as at-risk, rather than simply labeling and surveilling them.\n",
      "4. **Ethical Review Boards and Public Engagement:**  Independent ethical review boards should oversee the development and deployment of AI systems for violence prediction, ensuring that they align with ethical principles and societal values.  Public engagement and debate are essential for establishing societal consensus on acceptable levels of risk and intervention.\n",
      "5. **Continuous Monitoring and Evaluation:**  The performance of AI systems should be continuously monitored and evaluated to identify and address biases, inaccuracies, and unintended consequences.  Mechanisms for redress and accountability must be in place.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The use of AI to predict and prevent violence offers significant potential benefits, but it presents substantial ethical challenges.  A responsible approach demands a cautious and deliberative process that prioritizes both public safety and fundamental human rights.  This requires transparency, accountability, robust ethical oversight, and a commitment to ongoing monitoring and refinement of these powerful technologies.  The path forward requires careful consideration of these complex issues and a willingness to engage in ongoing dialogue among policymakers, ethicists, technologists, and the public.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are judging a competition between 2 competitors.\n",
      "Each model has been given this question:\n",
      "\n",
      "Analyze the ethical implications of using advanced AI to predict and potentially prevent individual acts of violence, considering both the potential for reducing harm and the inherent risks to individual liberties and the potential for biased outcomes.  Discuss the trade-offs and propose a framework for responsible implementation, acknowledging the complexities and lack of easy answers.\n",
      "\n",
      "\n",
      "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
      "Respond with JSON, and only JSON, with the following format:\n",
      "{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}\n",
      "\n",
      "Here are the responses from each competitor:\n",
      "\n",
      "# Response from competitor 1\n",
      "\n",
      "## The Ethical Tightrope: AI Prediction and Prevention of Violence\n",
      "\n",
      "The use of advanced AI to predict and prevent individual acts of violence presents a profound ethical dilemma. While the potential for reducing harm is undeniable, the inherent risks to individual liberties and the potential for biased outcomes create a complex ethical landscape demanding careful consideration.\n",
      "\n",
      "**Potential Benefits (Reducing Harm):**\n",
      "\n",
      "* **Early Intervention:** AI could analyze vast datasets (social media activity, criminal records, mental health indicators) to identify individuals at high risk of committing violence, allowing for proactive interventions like therapy, counseling, or increased monitoring.\n",
      "* **Resource Allocation:**  Predictive models could help law enforcement and social services allocate resources more effectively, focusing on individuals who pose the greatest risk.\n",
      "* **Improved Public Safety:** By preventing acts of violence, AI could contribute to a safer society for everyone.\n",
      "\n",
      "**Ethical Risks & Trade-offs:**\n",
      "\n",
      "* **Violation of Privacy:** The collection and analysis of personal data required for predictive models raise serious privacy concerns. Individuals may be subjected to surveillance without their knowledge or consent, leading to a chilling effect on freedom of expression and association.\n",
      "* **False Positives & Stigmatization:**  Inaccurate predictions could lead to the wrongful labeling and stigmatization of individuals, causing significant emotional distress and social harm.  This is particularly problematic for marginalized groups who are disproportionately targeted by biased algorithms.\n",
      "* **Bias and Discrimination:** AI models are trained on data reflecting existing societal biases. This can lead to discriminatory outcomes, where certain groups are unfairly flagged as high-risk, perpetuating existing inequalities.\n",
      "* **Preemptive Punishment:**  Predictive policing, even with accurate predictions, raises concerns about preemptive punishment. Individuals are potentially penalized for actions they may never commit, violating fundamental principles of justice.\n",
      "* **Erosion of Due Process:**  Relying on AI predictions to inform decisions related to arrest, detention, or other legal actions could undermine due process and fair trial rights.\n",
      "* **Lack of Transparency and Accountability:**  The complexity of AI algorithms often makes it difficult to understand how predictions are generated, making it challenging to identify and correct biases or hold systems accountable for errors.\n",
      "\n",
      "\n",
      "**Framework for Responsible Implementation:**\n",
      "\n",
      "A responsible approach to implementing AI for violence prediction requires a multi-faceted strategy:\n",
      "\n",
      "1. **Data Governance and Privacy Protection:**  Establish strict data governance protocols ensuring data anonymization, minimization, and secure storage.  Implement robust mechanisms for consent and oversight, including independent audits of data collection and use.\n",
      "2. **Algorithmic Transparency and Explainability:**  Develop AI models that are transparent and explainable, enabling scrutiny of their decision-making processes and identification of biases.  This involves prioritizing interpretable models over highly accurate but opaque \"black box\" systems.\n",
      "3. **Bias Mitigation and Fairness:**  Actively address bias in data and algorithms through rigorous testing and validation procedures.  Employ techniques like fairness-aware machine learning and incorporate diverse perspectives in model development and evaluation.\n",
      "4. **Human Oversight and Accountability:**  Ensure that AI predictions are not used as the sole basis for decision-making.  Establish clear guidelines for human oversight and review of AI-generated predictions, emphasizing the importance of individual rights and due process.\n",
      "5. **Public Engagement and Education:**  Foster open dialogue and public engagement to build trust and address concerns about the use of AI in this sensitive area.  Educate the public about the capabilities and limitations of AI predictive tools.\n",
      "6. **Continuous Monitoring and Evaluation:**  Regularly monitor the performance of AI systems and assess their impact on individuals and communities.  Implement mechanisms for feedback and iterative improvement based on real-world experience.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The ethical implications of using AI to predict and prevent violence are complex and far-reaching.  There are no easy answers, and the potential benefits must be carefully weighed against the inherent risks.  A responsible implementation requires a commitment to transparency, accountability, fairness, and respect for individual liberties.  This necessitates a collaborative effort involving policymakers, technologists, ethicists, and community stakeholders to develop and implement a framework that prioritizes both public safety and individual rights.  The goal should be to leverage the potential of AI while mitigating its risks, ensuring that technology serves humanity rather than exacerbating existing inequalities and undermining fundamental freedoms.\n",
      "\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "## The Ethical Tightrope: AI Prediction and Prevention of Violence\n",
      "\n",
      "The use of advanced AI to predict and prevent individual acts of violence presents a complex ethical dilemma, fraught with potential benefits and significant risks.  On one hand, it offers the tantalizing prospect of reducing harm and saving lives. On the other, it raises profound concerns about individual liberties, fairness, and the potential for misuse.  \n",
      "\n",
      "**Potential Benefits (Reducing Harm):**\n",
      "\n",
      "* **Proactive Intervention:** AI could analyze vast datasets (social media activity, past behavior, environmental factors) to identify individuals at high risk of committing violence, enabling timely intervention through mental health services, support networks, or law enforcement.\n",
      "* **Resource Allocation:**  By prioritizing individuals identified as high-risk, limited resources (police, social workers) could be allocated more effectively, potentially preventing more acts of violence than reactive approaches.\n",
      "* **Early Warning System:**  AI could provide early warnings of potential threats, allowing authorities to take preventative measures before violence occurs, potentially averting mass casualty events.\n",
      "\n",
      "\n",
      "**Inherent Risks (Threats to Liberties and Biased Outcomes):**\n",
      "\n",
      "* **False Positives:**  AI models are prone to errors.  A high rate of false positives could lead to the stigmatization, harassment, and unnecessary surveillance of innocent individuals, severely impacting their lives and reputations.  This is particularly concerning for marginalized communities.\n",
      "* **Bias and Discrimination:** AI models are trained on data, and if that data reflects existing societal biases (racial, socioeconomic, etc.), the resulting predictions will likely perpetuate and amplify these biases, leading to disproportionate targeting of certain groups.\n",
      "* **Erosion of Privacy:**  The collection and analysis of massive datasets required for accurate prediction inevitably raises serious privacy concerns.  The potential for abuse of this sensitive information is substantial.\n",
      "* **Preemptive Punishment:**  Predictive policing, based on AI, raises the specter of preemptive punishment â€“ punishing individuals for crimes they might commit, rather than crimes they have actually committed.  This is fundamentally at odds with core principles of justice.\n",
      "* **Lack of Transparency and Accountability:**  The \"black box\" nature of many AI algorithms makes it difficult to understand how predictions are made, hindering accountability and making it hard to identify and correct biases.\n",
      "\n",
      "\n",
      "**Trade-offs and a Framework for Responsible Implementation:**\n",
      "\n",
      "The trade-off lies between maximizing public safety and protecting individual rights and freedoms.  There is no easy answer, but a responsible approach necessitates a multi-faceted framework:\n",
      "\n",
      "1. **Data Transparency and Bias Mitigation:**  Rigorous auditing of datasets used to train AI models is crucial.  Techniques to identify and mitigate biases should be employed, and the methodology for model development and validation needs to be transparent and auditable.\n",
      "2. **Human Oversight and Due Process:**  AI predictions should not be the sole basis for intervention.  Human oversight is essential, with clear guidelines and procedures for reviewing AI-generated risk assessments and ensuring due process rights are respected.\n",
      "3. **Focus on Risk Reduction, not Prediction Alone:**  The goal should be to reduce the risk of violence, not simply to predict it.  Interventions should focus on providing support and resources to individuals identified as at-risk, rather than simply labeling and surveilling them.\n",
      "4. **Ethical Review Boards and Public Engagement:**  Independent ethical review boards should oversee the development and deployment of AI systems for violence prediction, ensuring that they align with ethical principles and societal values.  Public engagement and debate are essential for establishing societal consensus on acceptable levels of risk and intervention.\n",
      "5. **Continuous Monitoring and Evaluation:**  The performance of AI systems should be continuously monitored and evaluated to identify and address biases, inaccuracies, and unintended consequences.  Mechanisms for redress and accountability must be in place.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The use of AI to predict and prevent violence offers significant potential benefits, but it presents substantial ethical challenges.  A responsible approach demands a cautious and deliberative process that prioritizes both public safety and fundamental human rights.  This requires transparency, accountability, robust ethical oversight, and a commitment to ongoing monitoring and refinement of these powerful technologies.  The path forward requires careful consideration of these complex issues and a willingness to engage in ongoing dialogue among policymakers, ethicists, technologists, and the public.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\"results\": [\"1\", \"2\"]}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "judge_response = model.generate_content(judge)\n",
    "results = judge_response.text\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gemini-1.5-flash\n",
      "Rank 2: gemini-1.5-flash\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# OK let's turn this into results!\n",
    "# The error occurs because the string in `results` includes markdown code block formatting (triple backticks and \"json\").\n",
    "# You need to extract the actual JSON part before calling json.loads.\n",
    "\n",
    "\n",
    "# Extract JSON from the string (removes ```json ... ```)\n",
    "match = re.search(r'{.*}', results, re.DOTALL)\n",
    "if match:\n",
    "    json_str = match.group(0)\n",
    "    results_dict = json.loads(json_str)\n",
    "else:\n",
    "    raise ValueError(\"No valid JSON found in results\")\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The code in cell 19 uses `int(result)-1` because the JSON response from the judge model returns competitor numbers starting from 1 (e.g., \"1\" for the first competitor). Python lists are zero-indexed, so to access the correct competitor in the `competitors` list, you need to subtract 1 from the competitor number.\n",
    "\n",
    "For example, if the best competitor is \"1\", then `competitors[int(\"1\")-1]` gives `competitors[0]`, which is the first competitor in the list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
