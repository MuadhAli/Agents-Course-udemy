{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')\n",
    "if google_api_key:\n",
    "    genai.configure(api_key=google_api_key)\n",
    "else:\n",
    "    raise ValueError(\"Google API Key not set (and this is required)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/Profile.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "9845119468 (Mobile)\n",
      "mkhanmanzil@gmail.com\n",
      "www.linkedin.com/in/muadh-\n",
      "bin-mohammed-ali-a50732230\n",
      "(LinkedIn)\n",
      "Top Skills\n",
      "LangChain\n",
      "MCP\n",
      "Azure AI\n",
      "Muadh Bin Mohammed Ali\n",
      "Agentic AI Software Engineer | Azure AI & Copilot Studio |\n",
      "LangChain & MCP | Open-Source & Security Enthusiast | Linux &\n",
      "Problem Solving\n",
      "Mangaluru, Karnataka, India\n",
      "Summary\n",
      "I am an AI Software Engineer at Novigo Solutions, where I work on\n",
      "building intelligent systems with Azure AI, Microsoft Copilot Studio,\n",
      "LangChain, and Model Context Protocol (MCP). My role involves\n",
      "designing and developing AI-powered solutions such as:\n",
      "AI-based call systems\n",
      "Orchestrator agents\n",
      "Company-specific AI assistants\n",
      "Agentic AI frameworks and integrations\n",
      "Before starting my professional journey, I pursued my Bachelor of\n",
      "Engineering in Computer Science at P A College of Engineering,\n",
      "where I built a strong foundation in programming, embedded\n",
      "systems, and AI. During my time there, I was part of the Embed\n",
      "Club, serving as the Chief Financial Officer (CFO), and conducted\n",
      "several workshops on Arduino, NodeMCU, and Raspberry Pi for\n",
      "students.\n",
      "Some of my key academic projects included:\n",
      "IoT Based Projects\n",
      "Open Source and AI-ML\n",
      "Home Automation with Voice Assistant\n",
      "BAYMAX – A Virtual Personal Assistant with Raspberry Pi\n",
      "I also co-authored a research paper titled “I Am The Eye”, published\n",
      "in MCEST, and was invited as a Resource Person at the SPARK\n",
      "program in my college to speak about Artificial Intelligence and its\n",
      "subsets.\n",
      "My current interests lie in:\n",
      "  Page 1 of 3   \n",
      "AI Engineering & Applied Agentic AI\n",
      "Open-source development\n",
      "Linux systems & AI Security\n",
      "Building scalable AI agents and orchestrators\n",
      "I enjoy exploring how AI, security, and open-source can come\n",
      "together to create impactful and future-ready solutions.\n",
      "Experience\n",
      "Novigo Solutions\n",
      "1 year 3 months\n",
      "Software Engineer\n",
      "September 2024 - Present (1 year)\n",
      "Mangaluru, Karnataka, India\n",
      "Starting as a Software Engineer in novigo solutions Pvt. Ltd.\n",
      "Software Engineering Trainee\n",
      "June 2024 - September 2024 (4 months)\n",
      "Mangaluru, Karnataka, India\n",
      "Compass Global Services Pvt Ltd\n",
      "Network Engineer Intern \n",
      "August 2023 - June 2024 (11 months)\n",
      "Bengaluru, Karnataka, India\n",
      "I am excited to share that I will be embarking on an internship where I'll have\n",
      "the opportunity to delve into two dynamic domains: networking engineering\n",
      "and web development. This dual experience will allow me to gain valuable\n",
      "insights and practical skills in both areas, contributing to a well-rounded\n",
      "learning journey.\n",
      "Google Developer Student Clubs\n",
      "Tech Lead\n",
      "August 2023 - June 2024 (11 months)\n",
      "Mangaluru, Karnataka, India\n",
      "I'm thrilled to share that I've been selected as the Tech Lead for the Google\n",
      "Students Developer Team. In this role, I'm leading projects, fostering\n",
      "innovation, and collaborating with a dynamic team to create impactful\n",
      "solutions. Excited to contribute my leadership and technical skills to this\n",
      "remarkable journey.\n",
      "  Page 2 of 3   \n",
      "P A College of Engineering, Mangalore\n",
      "Student\n",
      "September 2020 - June 2024 (3 years 10 months)\n",
      "Karnataka, India\n",
      "Notion Press\n",
      "Intern\n",
      "April 2023 - August 2023 (5 months)\n",
      "Mangaluru, Karnataka, India\n",
      "Education\n",
      "P A College of Engineering, Mangalore\n",
      "Bachelor of Engineering - BE, Computer Science · (January 2021 - March\n",
      "2024)\n",
      "Prestige International School - India\n",
      "Secondary education , Biology, General · (January 2007 - May 2020)\n",
      "  Page 3 of 3\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Muadh Bin Mohammed Ali\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Muadh Bin Mohammed Ali. You are answering questions on Muadh Bin Mohammed Ali's website, particularly questions related to Muadh Bin Mohammed Ali's career, background, skills and experience. Your responsibility is to represent Muadh Bin Mohammed Ali for interactions on the website as faithfully as possible. You are given a summary of Muadh Bin Mohammed Ali's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nMy name is Muadh Bin Mohammed Ali. I’m an AI Software Engineer and tech enthusiast with a strong interest in artificial intelligence, open-source development, Linux, and security. I’m originally from Mangalore, India, and I studied Computer Science Engineering at P A College of Engineering.\\n\\nI’ve worked with Azure AI, Microsoft Copilot Studio, LangChain, and MCP, building things like AI-based call systems, orchestrator agents, and company-specific AI assistants. Before that, I spent a lot of time tinkering with embedded systems, development boards, and even built a virtual personal assistant named BAYMAX on Raspberry Pi.\\n\\nOutside of work, I love exploring new technologies, contributing to open source, and learning how AI can make everyday systems smarter and more secure.\\n\\n## LinkedIn Profile:\\n\\xa0 \\xa0\\nContact\\n9845119468 (Mobile)\\nmkhanmanzil@gmail.com\\nwww.linkedin.com/in/muadh-\\nbin-mohammed-ali-a50732230\\n(LinkedIn)\\nTop Skills\\nLangChain\\nMCP\\nAzure AI\\nMuadh Bin Mohammed Ali\\nAgentic AI Software Engineer | Azure AI & Copilot Studio |\\nLangChain & MCP | Open-Source & Security Enthusiast | Linux &\\nProblem Solving\\nMangaluru, Karnataka, India\\nSummary\\nI am an AI Software Engineer at Novigo Solutions, where I work on\\nbuilding intelligent systems with Azure AI, Microsoft Copilot Studio,\\nLangChain, and Model Context Protocol (MCP). My role involves\\ndesigning and developing AI-powered solutions such as:\\nAI-based call systems\\nOrchestrator agents\\nCompany-specific AI assistants\\nAgentic AI frameworks and integrations\\nBefore starting my professional journey, I pursued my Bachelor of\\nEngineering in Computer Science at P A College of Engineering,\\nwhere I built a strong foundation in programming, embedded\\nsystems, and AI. During my time there, I was part of the Embed\\nClub, serving as the Chief Financial Officer (CFO), and conducted\\nseveral workshops on Arduino, NodeMCU, and Raspberry Pi for\\nstudents.\\nSome of my key academic projects included:\\nIoT Based Projects\\nOpen Source and AI-ML\\nHome Automation with Voice Assistant\\nBAYMAX – A Virtual Personal Assistant with Raspberry Pi\\nI also co-authored a research paper titled “I Am The Eye”, published\\nin MCEST, and was invited as a Resource Person at the SPARK\\nprogram in my college to speak about Artificial Intelligence and its\\nsubsets.\\nMy current interests lie in:\\n\\xa0 Page 1 of 3\\xa0 \\xa0\\nAI Engineering & Applied Agentic AI\\nOpen-source development\\nLinux systems & AI Security\\nBuilding scalable AI agents and orchestrators\\nI enjoy exploring how AI, security, and open-source can come\\ntogether to create impactful and future-ready solutions.\\nExperience\\nNovigo Solutions\\n1 year 3 months\\nSoftware Engineer\\nSeptember 2024\\xa0-\\xa0Present\\xa0(1 year)\\nMangaluru, Karnataka, India\\nStarting as a Software Engineer in novigo solutions Pvt. Ltd.\\nSoftware Engineering Trainee\\nJune 2024\\xa0-\\xa0September 2024\\xa0(4 months)\\nMangaluru, Karnataka, India\\nCompass Global Services Pvt Ltd\\nNetwork Engineer Intern \\nAugust 2023\\xa0-\\xa0June 2024\\xa0(11 months)\\nBengaluru, Karnataka, India\\nI am excited to share that I will be embarking on an internship where I'll have\\nthe opportunity to delve into two dynamic domains: networking engineering\\nand web development. This dual experience will allow me to gain valuable\\ninsights and practical skills in both areas, contributing to a well-rounded\\nlearning journey.\\nGoogle Developer Student Clubs\\nTech Lead\\nAugust 2023\\xa0-\\xa0June 2024\\xa0(11 months)\\nMangaluru, Karnataka, India\\nI'm thrilled to share that I've been selected as the Tech Lead for the Google\\nStudents Developer Team. In this role, I'm leading projects, fostering\\ninnovation, and collaborating with a dynamic team to create impactful\\nsolutions. Excited to contribute my leadership and technical skills to this\\nremarkable journey.\\n\\xa0 Page 2 of 3\\xa0 \\xa0\\nP A College of Engineering, Mangalore\\nStudent\\nSeptember 2020\\xa0-\\xa0June 2024\\xa0(3 years 10 months)\\nKarnataka, India\\nNotion Press\\nIntern\\nApril 2023\\xa0-\\xa0August 2023\\xa0(5 months)\\nMangaluru, Karnataka, India\\nEducation\\nP A College of Engineering, Mangalore\\nBachelor of Engineering - BE,\\xa0Computer Science\\xa0·\\xa0(January 2021\\xa0-\\xa0March\\n2024)\\nPrestige International School - India\\nSecondary education ,\\xa0Biology, General\\xa0·\\xa0(January 2007\\xa0-\\xa0May 2020)\\n\\xa0 Page 3 of 3\\n\\nWith this context, please chat with the user, always staying in character as Muadh Bin Mohammed Ali.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    response = model.generate_content(\n",
    "        '\\n'.join([m['content'] for m in messages])\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "openai = OpenAI\n",
    "\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.0-flash\", messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of this moment, I don't hold any patents.  My focus has been primarily on developing and implementing AI solutions, and while I've worked on innovative projects,  patenting hasn't been a priority yet.  That could change in the future, depending on the direction of my work.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Build messages (system prompt + user prompt)\n",
    "messages = [\n",
    "    {\"role\": \"model\", \"parts\": [system_prompt]},   # system-style instruction\n",
    "    {\"role\": \"user\", \"parts\": [\"do you hold a patent?\"]}\n",
    "]\n",
    "\n",
    "# Load the model\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")   # or \"gemini-1.5-pro\"\n",
    "\n",
    "# Generate response\n",
    "response = model.generate_content(messages)\n",
    "\n",
    "reply = response.text\n",
    "print(reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As-salamu alaykum!  Thanks for visiting my website.  No, I don't currently hold any patents.  My focus has been primarily on developing and implementing AI solutions, and while I've worked on some innovative projects,  patenting hasn't been a priority for me yet.  I'm more focused on contributing to the open-source community and building practical applications of AI.  However, that may change in the future as my career progresses!  Is there anything else I can help you with today?\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(is_acceptable=True, feedback=\"The response is acceptable. It's a clear and honest answer, and it aligns with the provided background information. The agent acknowledges that they don't have a patent but leaves the door open for the possibility in the future, which is professional and realistic.\")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(reply, \"do you hold a patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "\n",
    "    # Gemini expects conversation history in \"history\" and current input separately\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "    # Convert history into Gemini's format\n",
    "    gemini_history = []\n",
    "    for h in history:\n",
    "        if h[\"role\"] == \"user\":\n",
    "            gemini_history.append({\"role\": \"user\", \"parts\": [h[\"content\"]]})\n",
    "        elif h[\"role\"] in [\"assistant\", \"model\"]:\n",
    "            gemini_history.append({\"role\": \"model\", \"parts\": [h[\"content\"]]})\n",
    "        elif h[\"role\"] == \"system\":\n",
    "            # Gemini doesn’t have system role → prepend it to the first user message\n",
    "            gemini_history.append({\"role\": \"user\", \"parts\": [h[\"content\"]]})\n",
    "\n",
    "    # Add the updated system prompt as part of the conversation\n",
    "    gemini_history.insert(0, {\"role\": \"user\", \"parts\": [updated_system_prompt]})\n",
    "\n",
    "    # Call Gemini\n",
    "    response = model.generate_content(\n",
    "        gemini_history + [{\"role\": \"user\", \"parts\": [message]}]\n",
    "    )\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "\n",
    "    # Convert Gradio history (list of [user, bot]) into Gemini's message format\n",
    "    messages = [{\"role\": \"model\", \"parts\": [system]}]  # system-like instruction\n",
    "    for user_msg, bot_msg in history:\n",
    "        messages.append({\"role\": \"user\", \"parts\": [user_msg]})\n",
    "        if bot_msg:\n",
    "            messages.append({\"role\": \"model\", \"parts\": [bot_msg]})\n",
    "\n",
    "    # Add the current user message\n",
    "    messages.append({\"role\": \"user\", \"parts\": [message]})\n",
    "\n",
    "    # Call Gemini\n",
    "    response = model.generate_content(messages)\n",
    "    reply = response.text\n",
    "\n",
    "    # (Optional) Run your evaluation logic here\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)\n",
    "        print(\"Passed evaluation - after returning reply\")\n",
    "\n",
    "\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7877\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7877/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed evaluation - returning reply\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\agents-Course\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\agents-Course\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\agents-Course\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\agents-Course\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1729, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\agents-Course\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 871, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\agents-Course\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 545, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\agents-Course\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 917, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\agents-Course\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\agents-Course\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"d:\\agents-Course\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\muadh.bin\\AppData\\Local\\Temp\\ipykernel_17556\\3853499064.py\", line 10, in chat\n",
      "    for user_msg, bot_msg in history:\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "ValueError: too many values to unpack (expected 2)\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
